{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbac27a",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION - Logistic regression estimates the probability of an event occurring, such as voted or didn't vote, based on a given dataset of independent variables. Since the outcome is a probability, the dependent variable is bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "with open('logistic_regression_dataset.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        # grade1, grade2, admitted?\n",
    "        data.append([float(row[0]), float(row[1]), float(row[2])])\n",
    "\n",
    "print(data)\n",
    "\n",
    "X = np.array([d[:-1] for d in data])\n",
    "y = np.array([d[-1] for d in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79566f97",
   "metadata": {},
   "source": [
    "PREDICTION\n",
    "\n",
    "FORMULA: z = wx + b\n",
    "w, b - parameters (weights)\n",
    "\n",
    "f(x) = 1 / (1 + e**(-z))\n",
    "\n",
    "y_hat = [f(x) for i = 0...len(x)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xi, W, b):\n",
    "    # Predict the outcome y_hat of xi\n",
    "    # given the values of w and b\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "    # Compute z\n",
    "    \n",
    "    # Compute f(x)\n",
    "    \n",
    "    # Make sure to return the value of f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daebf0e1",
   "metadata": {},
   "source": [
    "Loss function - the error between predicted outcome and the actual outcome of a single input\n",
    "\n",
    "L(y_hat[i], y[i]) = -log(y_hat[i]) if y[i] == 1\n",
    "                    -log(1 - y_hat[i]) if y[i] == 0\n",
    "L(y_hat[i], y[i]) = -y[i] * log(y_hat[i]) - (1 - y[i]) * log(1 - y_hat[i])\n",
    "\n",
    "J(w, b) = -1/m * sum(L(y_hat[i], y[i]) for i = 0...len(x)-1)\n",
    "J(w, b) = -1/m * sum((y[i] * log(y_hat[i]) + (1 - y[i]) * log(1 - y_hat[i])) for i = 0...len(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745302df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost_function(y, y_hat):\n",
    "    m = len(y)\n",
    "    # ADD YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959492f",
   "metadata": {},
   "source": [
    "The GRADIENT DESCENT function is the same as linear regression, it just uses the updated prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c940623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparam\n",
    "alpha = 0.01\n",
    "\n",
    "def gradient_descent_step(W, b, X, y):\n",
    "    # Take a step in the steepest direction \n",
    "    # Compute the new values for W and b\n",
    "\n",
    "    global alpha\n",
    "    m = len(X)\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    # Compute y_hat\n",
    "    \n",
    "    # Compute wn = wn - α * 1/m * sum((y_hat[i] - y[i]) * x[i][n] for i = 0...m-1) for all weights\n",
    "    # and W = [w1, w2, ..., wn]\n",
    "    \n",
    "    # Compute b = b - α * 1/m * sum((y_hat[i] - y[i]) for i = 0...m-1)\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W, b, X, y):\n",
    "    m = len(X)\n",
    "    pred = [predict(X[i], W, b) for i in range(m)]\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred[i] = 0 if pred[i] < 0.5 else 1\n",
    "\n",
    "    return sum([pred[i] == y[i] for i in range(m)]) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET PARAMS\n",
    "W = [0 for _ in range(len(X[0]))]\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    W, b = gradient_descent_step(W, b, X, y)\n",
    "    print(W, b, accuracy(W, b, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rescaled = []\n",
    "for i in range(len(X[0])):\n",
    "    X_rescaled.append((X[:, i] - min(X[:, i])) / (max(X[:, i])- min(X[:, i])))\n",
    "\n",
    "X_rescaled = np.array(X_rescaled).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804484e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET PARAMS\n",
    "W = [0 for _ in range(len(X[0]))]\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34070421",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    W, b = gradient_descent_step(W, b, X_rescaled, y)\n",
    "    print(W, b, accuracy(W, b, X_rescaled, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
